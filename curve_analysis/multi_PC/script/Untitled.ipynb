{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysnptools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-94ce623aff6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madress_abs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/scripts\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Contains \"util\" and \"Polygenic_score\" packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPolygenic_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vcabeli/stage_n/curve_analysis/multi_PC/script/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpysnptools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpheno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpysnptools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnpreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pysnptools'"
     ]
    }
   ],
   "source": [
    "# %load script.py\n",
    "#! /usr/bin/env python2\n",
    "\n",
    "\n",
    "# import module\n",
    "import os\n",
    "adress_abs = os.getcwd()\n",
    "import sys\n",
    "sys.path.append(adress_abs+\"/scripts\") # Contains \"util\" and \"Polygenic_score\" packages\n",
    "import util\n",
    "from Polygenic_score import *\n",
    "\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clump_sorted_snps(bfile, assoc_file, snp_values, sorted_snps):\n",
    "\n",
    "    clump_out = assoc_file[:assoc_file.index(\".assoc\")] + \"_clump\"\n",
    "    cmd_plink_clump = \"plink --bfile {} --clump {} --clump-p1 1 --clump-p2 1 --clump-r2 0.2 --clump-kb 500 --out {} \".format(bfile,\n",
    "            assoc_file,\n",
    "            clump_out)\n",
    "    print cmd_plink_clump\n",
    "    p = subprocess.Popen(cmd_plink_clump, shell=True)\n",
    "    p.wait()\n",
    "\n",
    "    clumped_res = pd.read_table(clump_out+\".clumped\", delim_whitespace=True)\n",
    "    clumped_snps = set(clumped_res.SNP)\n",
    "\n",
    "    clumped_sorted_snps = np.array([i for i in sorted_snps if snp_values[i] in clumped_snps])\n",
    "    return clumped_sorted_snps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_strat(strat_file, strat):\n",
    "    pca_res = pd.read_table(strat_file, delim_whitespace=True, skiprows=1, header=None)\n",
    "    PCs_out = strat + \".PCs\"\n",
    "    with open(\"res_clumped_raw_genotype/\"+PCs_out, \"a\") as f:\n",
    "        f.write(', '.join(pca_res[0].values))\n",
    "        f.write('\\n')\n",
    "        f.write(str(pca_res[2].values.tolist())[1:-1])\n",
    "        f.write('\\n')\n",
    "        f.write(str(pca_res[3].values.tolist())[1:-1])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plink_prune(plink_bfile):\n",
    "\n",
    "    cmd_prune = \"\"\"plink --bfile {}      --exclude /home/vcabeli/Documents/data/high-LD-regions_37.txt           --range --indep-pairwise 50 5 0.2           --allow-extra-chr           --out {}\"\"\".format(plink_bfile, plink_bfile+\"_prune\")\n",
    "    p = subprocess.Popen(cmd_prune, shell=True)\n",
    "    assert(p.wait() == 0)\n",
    "\n",
    "    cmd_extract = \"\"\"plink --bfile {}           --extract {}           --allow-extra-chr           --chr 1-23           --make-bed           --out {}\"\"\".format(plink_bfile, plink_bfile + \"_prune.prune.in\", \n",
    "                             plink_bfile + \"_pruned\")\n",
    "    p = subprocess.Popen(cmd_extract, shell=True)\n",
    "    assert(p.wait() == 0)\n",
    "\n",
    "    print \"Wrote {} bed/bim/fam.\".format(plink_bfile + \"_pruned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def independent_snps(snps_list):\n",
    "    with open(\"non_pruned_list\", 'w') as f:\n",
    "        for snp in snps_list:\n",
    "            f.write(snp)\n",
    "            f.write(\"\\n\")\n",
    "    cmd_extract = \"plink --bfile {} --extract {} --make-bed --out {}\".format(\"training_set\",\n",
    "                                                                             \"non_pruned_list\",\n",
    "                                                                             \"snp_list\")\n",
    "    print cmd_extract\n",
    "    p = subprocess.Popen(cmd_extract, shell=True)\n",
    "    assert(p.wait()==0)\n",
    "\n",
    "    plink_prune(\"snp_list\")\n",
    "\n",
    "    t = pd.read_table(\"snp_list_pruned.bim\", delim_whitespace=True, header=None)\n",
    "    return t[1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_cov_file(pca_file):\n",
    "\n",
    "    cmd_sed = \"sed -i -e \\\"s/:/ /g\\\" -e \\\"s/\\s\\+/\\t/g\\\" -e \\\"s/^\\s\\+//g\\\" {}\".format(pca_file)\n",
    "    p = subprocess.Popen(cmd_sed, shell=True)\n",
    "    assert(p.wait()==0)\n",
    "    \n",
    "    cov_file = pca_file[:-9]+\".cov\"\n",
    "    cmd_print = \"printf \\\"FID\\tIID\\tPC1\\tPC2\\n\\\" > {}\".format(cov_file)\n",
    "    p = subprocess.Popen(cmd_print, shell=True)\n",
    "    assert(p.wait()==0)\n",
    "\n",
    "    cmd_tail = \"tail -n+2 {} | cut -f1-4 >> {}\".format(pca_file, cov_file)\n",
    "    p = subprocess.Popen(cmd_tail, shell=True)\n",
    "    assert(p.wait()==0)\n",
    "    \n",
    "    print \"Wrote {}\".format(cov_file)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pca(cov_file) :\n",
    "    \n",
    "    training_set_pca = pd.read_table(cov_file)\n",
    "    pheno_df = pd.DataFrame({'IID' : [pheno['iid'][i][1] for i in range(len(pheno['iid']))], 'pheno' : pheno['vals']})\n",
    "    training_set_pca = training_set_pca.merge(pheno_df, on=\"IID\", how=\"inner\")\n",
    "    \n",
    "    plt.scatter(data=training_set_pca[training_set_pca.pheno == 1], x=\"PC1\", y=\"PC2\", \n",
    "                label=\"controls\", s=15)\n",
    "    plt.scatter(data=training_set_pca[training_set_pca.pheno == 2], x=\"PC1\", y=\"PC2\",\n",
    "                label=\"cases\", s=15)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_quantile(quantiles, value):\n",
    "    idx = (np.abs(quantiles-value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_match(data, target_data, size, quantiles=100):\n",
    "    target_quantiles = target_data.quantile(np.linspace(0,1,quantiles)).values\n",
    "    quantile_map = {row.SNP : find_quantile(target_quantiles, row.MAF) for row in data.itertuples()}\n",
    "    len_quantiles = np.bincount(quantile_map.values())\n",
    "    \n",
    "    prop = [1./(quantiles)/len_quantiles[quantile_map[SNP]] for SNP in data.SNP]\n",
    "    \n",
    "    random_sample = np.random.choice(data.SNP, size, False, p=prop)\n",
    "    return random_sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "snp_data, pheno = util.load_data(\"/home/vcabeli/Documents/data/BP_final/BP.B37-final\")\n",
    "\n",
    "\n",
    "# #### Split training / testing datasets\n",
    "\n",
    "\n",
    "\n",
    "train_idces = np.random.choice(np.arange(snp_data.row_count), size=int(snp_data.row_count*0.5), replace=False)\n",
    "test_idces = np.setdiff1d(np.arange(snp_data.row_count), train_idces, assume_unique=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_sample_out = \"training_samples.keep\"\n",
    "\n",
    "with open(training_sample_out, 'w') as f:\n",
    "    for i in train_idces:\n",
    "        f.write(pheno['iid'][i][0] + \"\\t\" + pheno['iid'][i][1])\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### Build training set bed bim fam\n",
    "\n",
    "\n",
    "\n",
    "cmd_keep_plink = \"plink --bfile {} --keep {} --make-bed --out {}\".format(\"/home/vcabeli/Documents/data/BP_final/BP.B37-final\",\n",
    "                                                                         training_sample_out,\n",
    "                                                                         \"training_set\")\n",
    "print cmd_keep_plink\n",
    "p = subprocess.Popen(cmd_keep_plink, shell=True)\n",
    "p.wait()\n",
    "\n",
    "threshs = range(10, 70000, 300)\n",
    "thresh_LD = 5 #valeur de LD faible pour avoir SNPs indé\n",
    "\n",
    "\n",
    "\n",
    "cmd_extract_low_ld_snps = \"plink --bfile {} --extract {} --make-bed --out {}\".format(\"training_set\",\n",
    "                                                                                     \"L2_thresh_{}_BP.extract\".format(thresh_LD),\n",
    "                                                                                     \"low_ld\")\n",
    "print cmd_extract_low_ld_snps\n",
    "p = subprocess.Popen(cmd_extract_low_ld_snps, shell=True)\n",
    "assert(p.wait()==0)\n",
    "\n",
    "plink_prune(\"low_ld\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################PCA 1 ###########################\n",
    "\n",
    "cmd_smart_pca2 = \"smartpca.perl -i {} -a {} -b {} -o {} -p {} -e {} -l {} -k {} -t {} -m {}\".format(\"low_ld_pruned.bed\",\n",
    "                                                                                                   \"low_ld_pruned.bim\",\n",
    "                                                                                                   \"low_ld_pruned.fam\",\n",
    "                                                                                                   \"low_ld_pruned.pca\",\n",
    "                                                                                                   \"low_ld_pruned.plot\",\n",
    "                                                                                                   \"low_ld_pruned.eval\",\n",
    "                                                                                                   \"low_ld_pruned.log\",\n",
    "                                                                                                   2, 2, 0)\n",
    "print cmd_smart_pca2\n",
    "p = subprocess.Popen(cmd_smart_pca2, shell=True)\n",
    "p.wait()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "write_cov_file(\"low_ld_pruned.pca.evec\")\n",
    "save_strat(\"low_ld_pruned.pca.evec\", \"low_ld{}\".format(thresh_LD)) #OUTPUT LINE\n",
    "\n",
    "#écriture fichier covariables\n",
    "cmd_second_gwas = \"plink --bfile {} --logistic sex beta hide-covar --covar {} --covar-name PC1-PC2 --out {}\".format(\"training_set\",\n",
    "                                                                                                   \"low_ld_pruned.cov\",\n",
    "                                                                                                   \"low_ld\")\n",
    "print cmd_second_gwas\n",
    "p = subprocess.Popen(cmd_second_gwas, shell=True)\n",
    "p.wait()\n",
    "\n",
    "low_ld_res = pd.read_table(\"low_ld.assoc.logistic\", delim_whitespace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Compute Polygenic Risk Score\n",
    "\n",
    "\n",
    "sorted_snps_low_ld = np.argsort(low_ld_res.P)\n",
    "sorted_snps_low_ld = clump_sorted_snps(\"training_set\", \"low_ld.assoc.logistic\",\n",
    "                                       snp_data.col, sorted_snps_low_ld)\n",
    "\n",
    "\n",
    "prs_low_ld = polygen_score_sign(G_hw, sorted_snps_low_ld,\n",
    "                                threshs,\n",
    "                                test_idces, pheno,\n",
    "                                low_ld_res.BETA)\n",
    "with open(\"res_clumped_raw_genotype/prs_low_ld_{}.res\".format(thresh_LD), 'a') as f: #OUTPUT LINE\n",
    "    np.savetxt(f, prs_low_ld)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### RANDOM\n",
    "\n",
    "MAFs = pd.read_table(\"/home/vcabeli/Documents/data/BP_final/BP.B37-final_freqs.frq\", delim_whitespace=True)\n",
    "low_ld_snps = pd.read_table(\"L2_thresh_{}_BP.extract\".format(thresh_LD), header=None)\n",
    "low_ld_MAFs = MAFs[MAFs.SNP.isin(low_ld_snps.unstack().values)].MAF\n",
    "\n",
    "low_ld_snps = independent_snps(low_ld_snps[0].values)\n",
    "\n",
    "random_snps = np.random.choice(MAFs.SNP, len(low_ld_MAFs)*2, replace=False)\n",
    "random_snps = np.random.choice(independent_snps(random_snps), len(low_ld_snps), replace=False)\n",
    "\n",
    "\n",
    "with open(\"random_snps.extract\", 'w') as f:\n",
    "    for snp in random_snps:\n",
    "        f.write(snp)\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "cmd_extract_low_ld_snps = \"plink --bfile {} --extract {} --make-bed --out {}\".format(\"training_set\",\n",
    "                                                                                     #\"L2_thresh_{}_BP.extract\".format(thresh_LD),\n",
    "                                                                                     \"random_snps.extract\",\n",
    "                                                                                     \"low_ld\")\n",
    "print cmd_extract_low_ld_snps\n",
    "p = subprocess.Popen(cmd_extract_low_ld_snps, shell=True)\n",
    "assert(p.wait()==0)\n",
    "\n",
    "plink_prune(\"low_ld\")\n",
    "\n",
    "\n",
    "#########################################PCA 2 ###########################\n",
    "\n",
    "cmd_smart_pca2 = \"smartpca.perl -i {} -a {} -b {} -o {} -p {} -e {} -l {} -k {} -t {} -m {}\".format(\"low_ld_pruned.bed\",\n",
    "                                                                                                   \"low_ld_pruned.bim\",\n",
    "                                                                                                   \"low_ld_pruned.fam\",\n",
    "                                                                                                   \"low_ld_pruned.pca\",\n",
    "                                                                                                   \"low_ld_pruned.plot\",\n",
    "                                                                                                   \"low_ld_pruned.eval\",\n",
    "                                                                                                   \"low_ld_pruned.log\",\n",
    "                                                                                                   2, 2, 0)\n",
    "print cmd_smart_pca2\n",
    "p = subprocess.Popen(cmd_smart_pca2, shell=True)\n",
    "p.wait()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "write_cov_file(\"low_ld_pruned.pca.evec\")\n",
    "save_strat(\"low_ld_pruned.pca.evec\", \"random_{}\".format(thresh_LD)) #OUTPUT LINE\n",
    "\n",
    "#écriture fichier covariables\n",
    "cmd_second_gwas = \"plink --bfile {} --logistic sex beta hide-covar --covar {} --covar-name PC1-PC2 --out {}\".format(\"training_set\",\n",
    "                                                                                                   \"low_ld_pruned.cov\",\n",
    "                                                                                                   \"low_ld\")\n",
    "print cmd_second_gwas\n",
    "p = subprocess.Popen(cmd_second_gwas, shell=True)\n",
    "p.wait()\n",
    "\n",
    "low_ld_res = pd.read_table(\"low_ld.assoc.logistic\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "# ### Compute PRS\n",
    "\n",
    "\n",
    "\n",
    "sorted_snps_low_ld = np.argsort(low_ld_res.P)\n",
    "sorted_snps_low_ld = clump_sorted_snps(\"training_set\", \"low_ld.assoc.logistic\",\n",
    "                                       snp_data.col, sorted_snps_low_ld)\n",
    "\n",
    "\n",
    "prs_low_ld = polygen_score_sign(G_hw, sorted_snps_low_ld,\n",
    "                                threshs,\n",
    "                                test_idces, pheno,\n",
    "                                low_ld_res.BETA)\n",
    "with open(\"res_clumped_raw_genotype/prs_random_{}.res\".format(thresh_LD), 'a') as f: #OUTPUT LINE\n",
    "    np.savetxt(f, prs_low_ld)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
